{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/dylanedwards/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dylanedwards/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dylanedwards/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "positive_dict = set(opinion_lexicon.positive())\n",
    "negative_dict = set(opinion_lexicon.negative())\n",
    "positive_dict_stemmed = [PorterStemmer().stem(word) for word in positive_dict]\n",
    "negative_dict_stemmed = [PorterStemmer().stem(word) for word in negative_dict]\n",
    "\n",
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "def get_files_from_dir(directory):\n",
    "    return [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "\n",
    "def process_string_sentence(text):\n",
    "        englishStopwords = stopwords.words(\"english\")  # non-neccesary words\n",
    "        text = text.lower()  # case folding\n",
    "        # remove punctuation\n",
    "        text = \"\".join([char for char in text if char not in punctuation])\n",
    "        words = word_tokenize(text)\n",
    "        removed = [word for word in words if word not in englishStopwords]\n",
    "        stemmed = [PorterStemmer().stem(word) for word in removed]\n",
    "        stemmed_sentence = \" \".join(stemmed)\n",
    "        return stemmed_sentence\n",
    "\n",
    "def process_string(text):\n",
    "        englishStopwords = stopwords.words(\"english\")  # non-neccesary words\n",
    "        text = text.lower()  # case folding\n",
    "        # remove punctuation\n",
    "        text = \"\".join([char for char in text if char not in punctuation])\n",
    "        words = word_tokenize(text)\n",
    "        removed = [word for word in words if word not in englishStopwords]\n",
    "        return [\", \".join(removed)]\n",
    "\n",
    "\n",
    "def tokenize_files(files, dir):\n",
    "        cleaned_positive_files = []\n",
    "        for file in files:\n",
    "            file_path = str.format(\"{}/{}\", dir, file)\n",
    "            with open(file_path) as f:\n",
    "                raw_text = f.read()\n",
    "                cleaned_positive_files.append(process_string(raw_text))\n",
    "        return cleaned_positive_files\n",
    "\n",
    "def is_word_positive(word):\n",
    "        if word in positive_dict or word in positive_dict_stemmed:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def is_word_negative(word):\n",
    "    if word in negative_dict or word in negative_dict_stemmed:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def get_word_occurrences(tokenized_files):\n",
    "        word_occurrences = {}\n",
    "        word_occurrences[\"positive\"] = 0\n",
    "        word_occurrences[\"negative\"] = 0\n",
    "        total_num_words = 0\n",
    "        for file in tokenized_files:\n",
    "            # calc number exclams\n",
    "            # calc number pos/neg/words\n",
    "            for word in file:\n",
    "                if is_word_positive(word):\n",
    "                    word_occurrences[\"positive\"] += 1\n",
    "                if is_word_negative(word):\n",
    "                    word_occurrences[\"negative\"] += 1\n",
    "                if word not in word_occurrences:\n",
    "                    word_occurrences[word] = 0\n",
    "                word_occurrences[word] += 1\n",
    "                total_num_words += 1\n",
    "        return word_occurrences, total_num_words\n",
    "\n",
    "def get_raw_text_from_files(files: list, dir: str) -> list:\n",
    "    raw_text = []\n",
    "    for file in files:\n",
    "        file_path = str.format(\"{}/{}\", dir, file)\n",
    "        with open(file_path) as f:\n",
    "            file_text_in_lines = f.read()\n",
    "            raw_text.append(file_text_in_lines)\n",
    "    return raw_text\n",
    "\n",
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "\n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace contractions with their longer forms\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text)\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "    # remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Tokenize each word\n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [ordered, hp4705, bluetooth, keyboard, sent, t...\n",
       "1       [hoping, buy, small, handheld, vacuum, office,...\n",
       "2       [bought, 3, vtech, phones, house, since, begin...\n",
       "3       [canon, s2, batteries, last, almost, long, che...\n",
       "4       [purchased, 10, drives, mistake, flakey, disap...\n",
       "                              ...                        \n",
       "1995    [received, pretty, quick, sdsdqu, 1024, e10m, ...\n",
       "1996    [listed, product, description, pico, works, ip...\n",
       "1997    [bought, folks, christmas, took, time, hook, m...\n",
       "1998    [purchased, monster, cable, mp, hts, 1000, wit...\n",
       "1999    [fabulous, product, store, 700, photos, 8, meg...\n",
       "Name: Text_Cleaned, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "neg_data = np.array(get_raw_text_from_files(get_files_from_dir(\"./data/neg\"), \"data/neg\"))\n",
    "pos_data = np.array(get_raw_text_from_files(get_files_from_dir(\"./data/pos\"), \"data/pos\"))\n",
    "allData = np.concatenate((neg_data, pos_data))\n",
    "# making labels for the data, the first\n",
    "neg_labels = np.fromiter([0 for i in range(len(neg_data))], int)  # create negative labels\n",
    "pos_labels = np.fromiter([1 for i in range(len(pos_data))], int)  # create positive labels\n",
    "allLabels = np.concatenate((neg_labels, pos_labels))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Text_Cleaned'] = list(map(clean_text, allData))\n",
    "df['Text_Cleaned']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanedwards/Desktop/oxy-nlp/hw2/venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'O': 48,\n",
       " 'r': 82,\n",
       " 'd': 68,\n",
       " 'e': 69,\n",
       " ' ': 3,\n",
       " 'f': 70,\n",
       " 'o': 79,\n",
       " 'm': 77,\n",
       " 'y': 89,\n",
       " 'H': 41,\n",
       " 'P': 49,\n",
       " '4': 23,\n",
       " '7': 26,\n",
       " '0': 19,\n",
       " '5': 24,\n",
       " '.': 17,\n",
       " 'T': 53,\n",
       " 'h': 72,\n",
       " 'i': 73,\n",
       " 's': 83,\n",
       " 'n': 78,\n",
       " 't': 84,\n",
       " 'a': 65,\n",
       " 'b': 66,\n",
       " 'l': 76,\n",
       " 'u': 85,\n",
       " 'k': 75,\n",
       " 'w': 87,\n",
       " '-': 16,\n",
       " 'N': 47,\n",
       " 'I': 42,\n",
       " 'v': 86,\n",
       " 'c': 67,\n",
       " 'p': 80,\n",
       " 'g': 71,\n",
       " '\\n': 1,\n",
       " '(': 11,\n",
       " ',': 15,\n",
       " ')': 12,\n",
       " 'D': 37,\n",
       " 'V': 55,\n",
       " \"'\": 10,\n",
       " 'x': 88,\n",
       " 'W': 56,\n",
       " '3': 22,\n",
       " 'S': 52,\n",
       " 'A': 34,\n",
       " '\"': 5,\n",
       " 'q': 81,\n",
       " '?': 32,\n",
       " 'C': 36,\n",
       " '2': 21,\n",
       " '8': 27,\n",
       " 'z': 90,\n",
       " 'M': 46,\n",
       " '1': 20,\n",
       " 'Y': 58,\n",
       " 'L': 45,\n",
       " 'R': 51,\n",
       " 'j': 74,\n",
       " '!': 4,\n",
       " 'U': 54,\n",
       " 'Z': 59,\n",
       " '6': 25,\n",
       " 'K': 44,\n",
       " 'G': 40,\n",
       " 'E': 38,\n",
       " 'B': 35,\n",
       " 'J': 43,\n",
       " 'X': 57,\n",
       " '$': 7,\n",
       " ':': 29,\n",
       " '9': 28,\n",
       " '&': 9,\n",
       " ';': 30,\n",
       " '/': 18,\n",
       " 'F': 39,\n",
       " '+': 14,\n",
       " '%': 8,\n",
       " '#': 6,\n",
       " 'Q': 50,\n",
       " '[': 60,\n",
       " ']': 62,\n",
       " '\\t': 0,\n",
       " '_': 63,\n",
       " '*': 13,\n",
       " '@': 33,\n",
       " '~': 93,\n",
       " '=': 31,\n",
       " '\\\\': 61,\n",
       " '|': 91,\n",
       " 'ï¿½': 94,\n",
       " '\\x1a': 2,\n",
       " '}': 92,\n",
       " '`': 64}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# print(allData[0:2])\n",
    "bow_converter = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "y = bow_converter.fit_transform(df['Text_Cleaned'])\n",
    "# bigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[2,2])\n",
    "# tfidf_transform = text.TfidfTransformer(norm=None)\n",
    "# X_tfidf = tfidf_transform.fit_transform(X_bow)\n",
    "bow_converter.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def simple_logistic_classify(trainingData, trainingLabels, testingData, testingLabels, description, _C=1.0):\n",
    "    model = LogisticRegression(C=_C).fit(trainingData, trainingLabels)\n",
    "    score = model.score(testingData, testingLabels)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50150\n",
      "12601\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanedwards/Desktop/oxy-nlp/hw2/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 65 features, but LogisticRegression is expecting 68 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zz/yhmtwxfj489c3r52w8d8rqnr0000gn/T/ipykernel_14102/1861101821.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     29\u001B[0m     \u001B[0;31m# print(score)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 31\u001B[0;31m     \u001B[0mlabelPrediction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict_proba\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtestingData\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     32\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"label pred\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabelPrediction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/oxy-nlp/hw2/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001B[0m in \u001B[0;36mpredict_proba\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m   1668\u001B[0m         )\n\u001B[1;32m   1669\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0movr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1670\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_predict_proba_lr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1671\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1672\u001B[0m             \u001B[0mdecision\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecision_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/oxy-nlp/hw2/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001B[0m in \u001B[0;36m_predict_proba_lr\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    437\u001B[0m         \u001B[0mmulticlass\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mhandled\u001B[0m \u001B[0mby\u001B[0m \u001B[0mnormalizing\u001B[0m \u001B[0mthat\u001B[0m \u001B[0mover\u001B[0m \u001B[0mall\u001B[0m \u001B[0mclasses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    438\u001B[0m         \"\"\"\n\u001B[0;32m--> 439\u001B[0;31m         \u001B[0mprob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecision_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    440\u001B[0m         \u001B[0mexpit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprob\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mprob\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    441\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mprob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/oxy-nlp/hw2/venv/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001B[0m in \u001B[0;36mdecision_function\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    405\u001B[0m         \u001B[0mcheck_is_fitted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    406\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 407\u001B[0;31m         \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"csr\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    408\u001B[0m         \u001B[0mscores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msafe_sparse_dot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcoef_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdense_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintercept_\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mravel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/oxy-nlp/hw2/venv/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    574\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    575\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mno_val_X\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mcheck_params\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"ensure_2d\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 576\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_n_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    577\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    578\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/oxy-nlp/hw2/venv/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_check_n_features\u001B[0;34m(self, X, reset)\u001B[0m\n\u001B[1;32m    393\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    394\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mn_features\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_features_in_\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 395\u001B[0;31m             raise ValueError(\n\u001B[0m\u001B[1;32m    396\u001B[0m                 \u001B[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    397\u001B[0m                 \u001B[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: X has 65 features, but LogisticRegression is expecting 68 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "#Unsupervised smooth-inverse frequency (uSIF) weighted sentence embeddings model.\n",
    "\n",
    "\n",
    "for trainingIndex, testingIndex in kf.split(allData):\n",
    "    trainingData, testingData = allData[trainingIndex], allData[testingIndex]\n",
    "    trainingLabels, testingLabels = allLabels[trainingIndex], allLabels[testingIndex]\n",
    "\n",
    "    # train naive bayes model\n",
    "    # gnb = GaussianNB()\n",
    "    # gnb.fit(train_embed, trainingLabels)\n",
    "    bow_converter = CountVectorizer(tokenizer=lambda doc: doc)\n",
    "    trainingData = bow_converter.fit_transform(trainingData)\n",
    "    testingData = bow_converter.fit_transform(testingData)\n",
    "#     print(\"hi\")\n",
    "    print(trainingData.size)\n",
    "    print(testingData.size)\n",
    "    print(testingLabels.size)\n",
    "    model = LogisticRegression(C=1.0).fit(trainingData, trainingLabels)\n",
    "\n",
    "#     score = model.score(testingData, testingLabels)\n",
    "    # print(score)\n",
    "    #\n",
    "    labelPrediction = model.predict_proba(testingData)[:, 1]\n",
    "    print(\"label pred\", labelPrediction)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(testingLabels, labelPrediction)\n",
    "    average_precision = average_precision_score(testingLabels, labelPrediction)\n",
    "\n",
    "    disp = plot_precision_recall_curve(model, testingData, testingLabels)\n",
    "    disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                       'AP={0:0.2f}'.format(average_precision))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}