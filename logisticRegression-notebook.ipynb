{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/dylanedwards/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dylanedwards/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dylanedwards/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "positive_dict = set(opinion_lexicon.positive())\n",
    "negative_dict = set(opinion_lexicon.negative())\n",
    "positive_dict_stemmed = [PorterStemmer().stem(word) for word in positive_dict]\n",
    "negative_dict_stemmed = [PorterStemmer().stem(word) for word in negative_dict]\n",
    "\n",
    "def get_files_from_dir(directory):\n",
    "    return [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "\n",
    "def process_string_sentence(text):\n",
    "        englishStopwords = stopwords.words(\"english\")  # non-neccesary words\n",
    "        text = text.lower()  # case folding\n",
    "        # remove punctuation\n",
    "        text = \"\".join([char for char in text if char not in punctuation])\n",
    "        words = word_tokenize(text)\n",
    "        removed = [word for word in words if word not in englishStopwords]\n",
    "        stemmed = [PorterStemmer().stem(word) for word in removed]\n",
    "        stemmed_sentence = \" \".join(stemmed)\n",
    "        return stemmed_sentence\n",
    "\n",
    "def process_string(text):\n",
    "        englishStopwords = stopwords.words(\"english\")  # non-neccesary words\n",
    "        text = text.lower()  # case folding\n",
    "        # remove punctuation\n",
    "        text = \"\".join([char for char in text if char not in punctuation])\n",
    "        words = word_tokenize(text)\n",
    "        removed = [word for word in words if word not in englishStopwords]\n",
    "        return \" \".join(removed)\n",
    "\n",
    "\n",
    "def tokenize_files(files, dir):\n",
    "        cleaned_positive_files = []\n",
    "        for file in files:\n",
    "            file_path = str.format(\"{}/{}\", dir, file)\n",
    "            with open(file_path) as f:\n",
    "                raw_text = f.read()\n",
    "                cleaned_positive_files.append(process_string(raw_text))\n",
    "        return cleaned_positive_files\n",
    "\n",
    "def is_word_positive(word):\n",
    "        if word in positive_dict or word in positive_dict_stemmed:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def is_word_negative(word):\n",
    "    if word in negative_dict or word in negative_dict_stemmed:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def get_word_occurrences(tokenized_files):\n",
    "        word_occurrences = {}\n",
    "        word_occurrences[\"positive\"] = 0\n",
    "        word_occurrences[\"negative\"] = 0\n",
    "        total_num_words = 0\n",
    "        for file in tokenized_files:\n",
    "            # calc number exclams\n",
    "            # calc number pos/neg/words\n",
    "            for word in file:\n",
    "                if is_word_positive(word):\n",
    "                    word_occurrences[\"positive\"] += 1\n",
    "                if is_word_negative(word):\n",
    "                    word_occurrences[\"negative\"] += 1\n",
    "                if word not in word_occurrences:\n",
    "                    word_occurrences[word] = 0\n",
    "                word_occurrences[word] += 1\n",
    "                total_num_words += 1\n",
    "        return word_occurrences, total_num_words\n",
    "\n",
    "def get_raw_text_from_files(files: list, dir: str) -> list:\n",
    "    raw_text = []\n",
    "    for file in files:\n",
    "        file_path = str.format(\"{}/{}\", dir, file)\n",
    "        with open(file_path) as f:\n",
    "            file_text_in_lines = f.read()\n",
    "            raw_text.append(file_text_in_lines)\n",
    "    return raw_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ordered hp4705 bluetooth keyboard sent two nonbluetooth item send back hope get credit'\n",
      " 'hoping buy small handheld vacuum office computer hardware keyboads air inlets however datavac little power even full charge next useless cant recommend'\n",
      " 'bought 3 vtech phones house since begining battery problems really annoying make phone call phone doesnt work battery point time least one phones constantly reports low charge battery message means cant use phone course hear ring phone never room run pick phone since one location battery problem reviewers noted spite fact make minimal use phones leave charger time beginning used take battery phone put back doesnt quite work anymore tried get support cant get live person vtech number website suggest replacing batteries rechargeable many times really supposed buy new rechargeable baterries already replaced know wont buy vtech products'\n",
      " ...\n",
      " 'bought folks christmas took time hook making dvds simple everything recording tv recording movies shows stored dvr transferring home movies original vhs tapes im sure 300 could purchase higher end recorders 200 really cant go wrong flaw could find initial hookup according book didnt seem quite right taking whole 2 minutes look easy read instructions online everything went smoothly im still going give full 5 starsits 2 whole minutes ruined life really youre new technology youre going problemsbelieve mea'\n",
      " 'purchased monster cable mp hts 1000 without reading reviewswhich dont usually simple reason purchased hdtv samsung hls5687w trying sell 150 much plugged equipment monster first picture sound directv connection allows receive digital analog signals cable connection needs placed monster connection loop switching around cables picture cant really see improvement picture quality hd beautiful bought monster basically protection'\n",
      " 'fabulous product store 700 photos 8 mega pixel range happy purchased one trouble free']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "neg_data = np.array(tokenize_files(get_files_from_dir(\"./data/neg\"), \"data/neg\"))\n",
    "pos_data = np.array(tokenize_files(get_files_from_dir(\"./data/pos\"), \"data/pos\"))\n",
    "allData = np.concatenate((neg_data, pos_data))\n",
    "print(allData)\n",
    "# making labels for the data, the first\n",
    "neg_labels = np.fromiter([0 for i in range(len(neg_data))], int)  # create negative labels\n",
    "pos_labels = np.fromiter([1 for i in range(len(pos_data))], int)  # create positive labels\n",
    "allLabels = np.concatenate((neg_labels, pos_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ordered hp4705 bluetooth keyboard sent two nonbluetooth item send back hope get credit'\n",
      " 'hoping buy small handheld vacuum office computer hardware keyboads air inlets however datavac little power even full charge next useless cant recommend']\n",
      "None\n",
      "28\n",
      "[' ', '0', '4', '5', '7', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanedwards/Desktop/oxy-nlp/hw2/venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print(allData[0:2])\n",
    "bow_converter = CountVectorizer(tokenizer=lambda doc: doc)\n",
    "y = bow_converter.fit_transform(allData[0:2])\n",
    "# bigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[2,2])\n",
    "# tfidf_transform = text.TfidfTransformer(norm=None)\n",
    "# X_tfidf = tfidf_transform.fit_transform(X_bow)\n",
    "print(bow_converter.get_stop_words())\n",
    "words = bow_converter.get_feature_names()\n",
    "print(len(words))\n",
    "print(words)\n",
    "print(len(bow_converter.vocabulary_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def simple_logistic_classify(trainingData, trainingLabels, testingData, testingLabels, description, _C=1.0):\n",
    "    model = LogisticRegression(C=_C).fit(trainingData, trainingLabels)\n",
    "    score = model.score(testingData, testingLabels)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "uSIF_model = uSIF(glove, workers=2, lang_freq=\"en\")\n",
    "#Unsupervised smooth-inverse frequency (uSIF) weighted sentence embeddings model.\n",
    "\n",
    "\n",
    "for trainingIndex, testingIndex in kf.split(allData):\n",
    "    trainingData, testingData = allData[trainingIndex], allData[testingIndex]\n",
    "    trainingLabels, testingLabels = allLabels[trainingIndex], allLabels[testingIndex]\n",
    "\n",
    "    print(\"trainingLabels:\", trainingLabels)\n",
    "    # train naive bayes model\n",
    "    # gnb = GaussianNB()\n",
    "    # gnb.fit(train_embed, trainingLabels)\n",
    "    model = LogisticRegression(C=_C).fit(trainingData, trainingLabels)\n",
    "    score = model.score(testingData, testingLabels)\n",
    "\n",
    "    labelPrediction = gnb.predict_proba(test_embed)[:, 1]\n",
    "    print(\"label pred\", labelPrediction)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(testingLabels, labelPrediction)\n",
    "    average_precision = average_precision_score(testingLabels, labelPrediction)\n",
    "\n",
    "    disp = plot_precision_recall_curve(gnb, test_embed, testingLabels)\n",
    "    disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                       'AP={0:0.2f}'.format(average_precision))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}